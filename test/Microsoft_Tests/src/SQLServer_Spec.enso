from Standard.Base import all
import Standard.Base.Errors.Illegal_Argument.Illegal_Argument
import Standard.Base.Errors.Illegal_State.Illegal_State
import Standard.Base.Runtime.Ref.Ref

from Standard.Table import Table, Value_Type, Aggregate_Column, Bits, expr
from Standard.Table.Errors import Invalid_Column_Names, Inexact_Type_Coercion, Duplicate_Output_Column_Names

import Standard.Database.DB_Column.DB_Column
import Standard.Database.DB_Table.DB_Table
import Standard.Database.SQL_Type.SQL_Type
import Standard.Database.Internal.Replace_Params.Replace_Params
from Standard.Database import all
from Standard.Database.Errors import all

from Standard.Snowflake import all

from Standard.Test import all
import Standard.Test.Test_Environment

import enso_dev.Table_Tests
import enso_dev.Table_Tests.Database.Common.Common_Spec
import enso_dev.Table_Tests.Database.Transaction_Spec
import enso_dev.Table_Tests.Database.Upload_Spec
import enso_dev.Table_Tests.Database.Helpers.Name_Generator
import enso_dev.Table_Tests.Common_Table_Operations
from enso_dev.Table_Tests.Common_Table_Operations.Util import all
from enso_dev.Table_Tests.Database.Types.Postgres_Type_Mapping_Spec import default_text
from enso_dev.Table_Tests.Database.Postgres_Spec import Basic_Test_Data, Postgres_Tables_Data

import enso_dev.Base_Tests.Network.Enso_Cloud.Cloud_Tests_Setup.Cloud_Tests_Setup

type Snowflake_Info_Data
    Value ~data

    connection self = self.data.at 0
    tinfo self = self.data.at 1
    t self = self.data.at 2

    setup create_connection_fn = Snowflake_Info_Data.Value <|
        connection = create_connection_fn Nothing
        tinfo = Name_Generator.random_name "Tinfo"
        connection.execute_update 'CREATE TEMPORARY TABLE "'+tinfo+'" ("strs" VARCHAR, "ints" NUMBER(38,0), "bools" BOOLEAN, "doubles" FLOAT8)'
        t = connection.query (SQL_Query.Table_Name tinfo)
        row1 = ["a", Nothing, False, 1.2]
        row2 = ["abc", Nothing, Nothing, 1.3]
        row3 = ["def", 42, True, 1.4]
        Panic.rethrow <|
            t.update_rows (Table.from_rows ["strs", "ints", "bools", "doubles"] [row1, row2, row3]) update_action=Update_Action.Insert
        [connection, tinfo, t]

    teardown self =
        self.connection.execute_update 'DROP TABLE "'+self.tinfo+'"'
        self.connection.close


snowflake_specific_spec suite_builder create_connection_fn db_name setup =
    table_builder = setup.table_builder
    materialize = setup.materialize

    suite_builder.group "[Snowflake] Schemas and Databases" group_builder->
        data = Basic_Test_Data.setup create_connection_fn

        group_builder.teardown <|
            data.teardown

        group_builder.specify "should be able to get current database and list databases" <|
            data.connection.database.equals_ignore_case db_name . should_be_true
            data.connection.databases.length . should_not_equal 0
            data.connection.databases.find (name-> name.equals_ignore_case db_name) . should_succeed
            Meta.is_same_object data.connection (data.connection.set_database db_name) . should_be_true

        group_builder.specify "should be able to get current schema and list schemas" <|
            data.connection.schema.equals_ignore_case "public" . should_be_true
            data.connection.schemas.length . should_not_equal 0
            data.connection.schemas.find (name-> name.equals_ignore_case "public") . should_succeed
            Meta.is_same_object data.connection (data.connection.set_schema "public") . should_be_true

        group_builder.specify "should allow changing schema" pending="TODO?" <|
            new_connection = data.connection.set_schema "information_schema"
            new_schema = new_connection.read (SQL_Query.Raw_SQL "SELECT current_schema()") . at 0 . to_vector . first
            new_schema . should_equal "information_schema"

        group_builder.specify "should allow changing database" <|
            databases = data.connection.databases.filter d->((d!=db_name) && (d!='rdsadmin'))
            pending_database = if databases.length != 0 then Nothing else "Cannot test changing database unless two databases defined."
            case pending_database of
                Nothing ->
                    new_connection = data.connection.set_database databases.first
                    new_database = new_connection.read (SQL_Query.Raw_SQL "SELECT current_database()") . at 0 . to_vector . first
                    new_database . should_equal databases.first
                # Nop - skip the test
                _ -> Nothing

    suite_builder.group "[Snowflake] Tables and Table Types" group_builder->
        data = Postgres_Tables_Data.setup create_connection_fn

        group_builder.teardown <|
            data.teardown

        group_builder.specify "should be able to list table types" <|
            table_types = data.connection.table_types
            table_types.length . should_not_equal 0
            table_types.contains "TABLE" . should_be_true
            table_types.contains "VIEW" . should_be_true

        group_builder.specify "should be able to list tables" <|
            tables = data.connection.tables
            tables.row_count . should_not_equal 0
            tables.columns.map .name . should_equal ["Database", "Schema", "Name", "Type", "Description"]

            table_names = tables.at "Name" . to_vector
            table_names.should_contain data.tinfo
            table_names.should_contain data.vinfo
            table_names.should_contain data.temporary_table

        group_builder.specify "should be able to filter tables by name" <|
            tables = data.connection.tables data.tinfo
            tables.row_count . should_equal 1
            tables.at "Database" . to_vector . at 0 . equals_ignore_case db_name . should_be_true
            tables.at "Schema" . to_vector . at 0 . equals_ignore_case "public" . should_be_true
            tables.at "Name" . to_vector . at 0 . should_equal data.tinfo
            tables.at "Type" . to_vector . at 0 . should_equal "TABLE"

            data.connection.tables "TestT_ble%" . row_count . should_equal 1
            data.connection.tables "Temporary%ble%" . row_count . should_equal 1
            data.connection.tables "Temporary%ble%" . at "Type" . to_vector . should_equal ["TEMPORARY"]
            data.connection.tables "N_nexistent%" . row_count . should_equal 0

        group_builder.specify "should be able to filter tables by type" <|
            tables = data.connection.tables types=["VIEW"]
            tables.row_count . should_not_equal 0
            tables.at "Name" . to_vector . contains data.tinfo . should_be_false
            tables.at "Name" . to_vector . contains data.vinfo . should_be_true


    suite_builder.group "[Snowflake] Info" group_builder->
        data = Snowflake_Info_Data.setup create_connection_fn

        group_builder.teardown <|
            data.teardown

        group_builder.specify "should return Table information" <|
            i = data.t.column_info
            i.at "Column" . to_vector . should_equal ["strs", "ints", "bools", "doubles"]
            i.at "Items Count" . to_vector . should_equal [3, 1, 2, 3]
            i.at "Value Type" . to_vector . should_equal [default_text, Value_Type.Integer, Value_Type.Boolean, Value_Type.Float]

        group_builder.specify "should return Table information, also for aggregated results" <|
            i = data.t.aggregate columns=[Aggregate_Column.Concatenate "strs", Aggregate_Column.Sum "ints", Aggregate_Column.Count_Distinct "bools"] . column_info
            i.at "Column" . to_vector . should_equal ["Concatenate strs", "Sum ints", "Count Distinct bools"]
            i.at "Items Count" . to_vector . should_equal [1, 1, 1]
            i.at "Value Type" . to_vector . should_equal [default_text, Value_Type.Decimal, Value_Type.Integer]

        group_builder.specify "should infer standard types correctly" <|
            data.t.at "strs" . value_type . is_text . should_be_true
            data.t.at "ints" . value_type . is_integer . should_be_true
            data.t.at "bools" . value_type . is_boolean . should_be_true
            data.t.at "doubles" . value_type . is_floating_point . should_be_true

        group_builder.specify "should preserve Snowflake types when table is materialized, where possible" pending="TODO" <|
            name = Name_Generator.random_name "types-test"
            Problems.assume_no_problems <|
                data.connection.execute_update 'CREATE TEMPORARY TABLE "'+name+'" ("int4" int4, "int2" int2, "txt-limited" varchar(10), "txt-fixed" char(3))'
            t1 = data.connection.query (SQL_Query.Table_Name name)
            t1.at "int4" . value_type . should_equal (Value_Type.Integer Bits.Bits_32)
            t1.at "int2" . value_type . should_equal (Value_Type.Integer Bits.Bits_16)
            t1.at "txt-limited" . value_type . should_equal (Value_Type.Char size=10 variable_length=True)
            t1.at "txt-fixed" . value_type . should_equal (Value_Type.Char size=3 variable_length=False)

            in_memory = t1.read
            in_memory.at "int4" . value_type . should_equal (Value_Type.Integer Bits.Bits_32)
            in_memory.at "int2" . value_type . should_equal (Value_Type.Integer Bits.Bits_16)
            in_memory.at "txt-limited" . value_type . should_equal (Value_Type.Char size=10 variable_length=True)
            in_memory.at "txt-fixed" . value_type . should_equal (Value_Type.Char size=3 variable_length=False)

    suite_builder.group "[Snowflake] Dialect-specific codegen" group_builder->
        data = Snowflake_Info_Data.setup create_connection_fn

        group_builder.teardown <|
            data.teardown

        group_builder.specify "should generate queries for the Distinct operation" <|
            t = data.connection.query (SQL_Query.Table_Name data.tinfo)
            code_template = 'SELECT "{Tinfo}"."strs" AS "strs", "{Tinfo}"."ints" AS "ints", "{Tinfo}"."bools" AS "bools", "{Tinfo}"."doubles" AS "doubles" FROM (SELECT DISTINCT ON ("{Tinfo}_inner"."strs") "{Tinfo}_inner"."strs" AS "strs", "{Tinfo}_inner"."ints" AS "ints", "{Tinfo}_inner"."bools" AS "bools", "{Tinfo}_inner"."doubles" AS "doubles" FROM (SELECT "{Tinfo}"."strs" AS "strs", "{Tinfo}"."ints" AS "ints", "{Tinfo}"."bools" AS "bools", "{Tinfo}"."doubles" AS "doubles" FROM "{Tinfo}" AS "{Tinfo}") AS "{Tinfo}_inner") AS "{Tinfo}"'
            expected_code = code_template.replace "{Tinfo}" data.tinfo
            t.distinct ["strs"] . to_sql . prepare . should_equal [expected_code, []]

    suite_builder.group "[Snowflake] Table.aggregate should correctly infer result types" group_builder->
        data = Snowflake_Info_Data.setup create_connection_fn

        group_builder.teardown <|
            data.teardown

        group_builder.specify "Concatenate, Shortest and Longest" <|
            r = data.t.aggregate columns=[Aggregate_Column.Concatenate "txt", Aggregate_Column.Shortest "txt", Aggregate_Column.Longest "txt"]
            r.columns.at 0 . value_type . should_equal default_text
            r.columns.at 1 . value_type . should_equal default_text
            r.columns.at 2 . value_type . should_equal default_text

        group_builder.specify "Counts" <|
            r = data.t.aggregate columns=[Aggregate_Column.Count, Aggregate_Column.Count_Empty "txt", Aggregate_Column.Count_Not_Empty "txt", Aggregate_Column.Count_Distinct "i1", Aggregate_Column.Count_Not_Nothing "i2", Aggregate_Column.Count_Nothing "i3"]
            r.column_count . should_equal 6
            r.columns.each column->
                column.value_type . should_equal Value_Type.Integer

        group_builder.specify "Sum" <|
            r = data.t.aggregate columns=[Aggregate_Column.Sum "i1", Aggregate_Column.Sum "i2", Aggregate_Column.Sum "i3", Aggregate_Column.Sum "i4", Aggregate_Column.Sum "r1", Aggregate_Column.Sum "r2"]
            r.columns.at 0 . value_type . should_equal Value_Type.Integer
            r.columns.at 1 . value_type . should_equal Value_Type.Integer
            r.columns.at 2 . value_type . should_equal Value_Type.Decimal
            r.columns.at 3 . value_type . should_equal Value_Type.Decimal
            r.columns.at 4 . value_type . should_equal (Value_Type.Float Bits.Bits_32)
            r.columns.at 5 . value_type . should_equal (Value_Type.Float Bits.Bits_64)

        group_builder.specify "Average" <|
            r = data.t.aggregate columns=[Aggregate_Column.Average "i1", Aggregate_Column.Average "i2", Aggregate_Column.Average "i3", Aggregate_Column.Average "i4", Aggregate_Column.Average "r1", Aggregate_Column.Average "r2"]
            r.columns.at 0 . value_type . should_equal Value_Type.Decimal
            r.columns.at 1 . value_type . should_equal Value_Type.Decimal
            r.columns.at 2 . value_type . should_equal Value_Type.Decimal
            r.columns.at 3 . value_type . should_equal Value_Type.Decimal
            r.columns.at 4 . value_type . should_equal Value_Type.Float
            r.columns.at 5 . value_type . should_equal Value_Type.Float


    suite_builder.group "[Snowflake] Warning/Error handling" group_builder->
        data = Basic_Test_Data.setup create_connection_fn

        group_builder.teardown <|
            data.teardown

        group_builder.specify "query warnings should be propagated" <|
            long_name = (Name_Generator.random_name "T") + ("a" * 100)
            r = data.connection.execute_update 'CREATE TEMPORARY TABLE "'+long_name+'" ("A" VARCHAR)'
            w1 = Problems.expect_only_warning SQL_Warning r
            # The display text may itself be truncated, so we just check the first words.
            w1.to_display_text . should_contain "identifier"
            # And check the full message for words that could be truncated in short message.
            w1.message . should_contain "truncated to"

            table = data.connection.query (SQL_Query.Raw_SQL 'SELECT 1 AS "'+long_name+'"')
            w2 = Problems.expect_only_warning SQL_Warning table
            w2.message . should_contain "truncated"
            effective_name = table.column_names . at 0
            effective_name . should_not_equal long_name
            long_name.should_contain effective_name

        group_builder.specify "is capable of handling weird tables" <|
            data.connection.execute_update 'CREATE TEMPORARY TABLE "empty-column-name" ("" VARCHAR)' . should_fail_with SQL_Error

            Problems.assume_no_problems <|
                data.connection.execute_update 'CREATE TEMPORARY TABLE "clashing-unicode-names" ("ś" VARCHAR, "s\u0301" INTEGER)'
            Problems.assume_no_problems <|
                data.connection.execute_update 'INSERT INTO "clashing-unicode-names" VALUES (\'A\', 2)'
            t2 = data.connection.query (SQL_Query.Table_Name "clashing-unicode-names")
            Problems.expect_only_warning Duplicate_Output_Column_Names t2
            t2.column_names . should_equal ["ś", "ś 1"]
            m2 = t2.read
            m2.at "ś"   . to_vector . should_equal ["A"]
            m2.at "ś 1" . to_vector . should_equal [2]

            r3 = data.connection.query 'SELECT 1 AS "A", 2 AS "A"'
            r3.should_fail_with Illegal_Argument
            r3.catch.cause . should_be_a Duplicate_Output_Column_Names

            r4 = data.connection.query 'SELECT 1 AS ""'
            r4.should_fail_with SQL_Error

    suite_builder.group "[Snowflake] Edge Cases" group_builder->
        data = Basic_Test_Data.setup create_connection_fn

        group_builder.teardown <|
            data.teardown

        group_builder.specify "materialize should respect the overridden type" pending="TODO" <|
            t0 = table_builder [["x", [False, True, False]], ["A", ["a", "b", "c"]], ["B", ["xyz", "abc", "def"]]] connection=data.connection
            t1 = t0 . cast "A" (Value_Type.Char size=1 variable_length=False) . cast "B" (Value_Type.Char size=3 variable_length=False)

            x = t1.at "x"
            a = t1.at "A"
            b = t1.at "B"
            a.value_type.should_equal (Value_Type.Char size=1 variable_length=False)
            b.value_type.should_equal (Value_Type.Char size=3 variable_length=False)

            c = x.iif a b
            c.to_vector.should_equal ["xyz", "b", "def"]
            Test.with_clue "c.value_type="+c.value_type.to_display_text+": " <|
                c.value_type.variable_length.should_be_true

            d = materialize c
            d.to_vector.should_equal ["xyz", "b", "def"]
            Test.with_clue "d.value_type="+d.value_type.to_display_text+": " <|
                d.value_type.variable_length.should_be_true

        group_builder.specify "should be able to round-trip a BigInteger column" pending="TODO" <|
            x = 2^70
            m1 = Table.new [["X", [10, x]]]
            m1.at "X" . value_type . should_be_a (Value_Type.Decimal ...)

            t1 = m1.select_into_database_table data.connection (Name_Generator.random_name "BigInteger") primary_key=[] temporary=True
            t1.at "X" . value_type . should_be_a (Value_Type.Decimal ...)
            t1.at "X" . value_type . scale . should_equal 0
            # If we want to enforce the scale, Postgres requires us to enforce a precision too, so we use the biggest one we can:
            t1.at "X" . value_type . precision . should_equal 1000
            w1 = Problems.expect_only_warning Inexact_Type_Coercion t1
            w1.requested_type . should_equal (Value_Type.Decimal precision=Nothing scale=0)
            w1.actual_type . should_equal (Value_Type.Decimal precision=1000 scale=0)

            v1x = t1.at "X" . to_vector
            v1x.should_equal [10, x]
            v1x.each e-> Test.with_clue "("+e.to_text+"): " <| e.should_be_a Integer

            t2 = t1.set (expr "[X] + 10") "Y"
            t2.at "X" . value_type . should_be_a (Value_Type.Decimal ...)
            t2.at "Y" . value_type . should_be_a (Value_Type.Decimal ...)
            # Unfortunately, performing operations on a Decimal column in postgres can lose information about it being an integer column.
            t2.at "Y" . value_type . scale . should_equal Nothing
            t2.at "X" . to_vector . should_equal [10, x]
            # Only works by approximation:
            t2.at "Y" . to_vector . should_equal [20, x+10]
            t2.at "Y" . cast Value_Type.Char . to_vector . should_equal ["20", (x+10).to_text]

            m2 = t2.remove_warnings.read
            m2.at "X" . value_type . should_be_a (Value_Type.Decimal ...)
            # As noted above - once operations are performed, the scale=0 may be lost and the column will be approximated as a float.
            m2.at "Y" . value_type . should_equal Value_Type.Float
            m2.at "X" . to_vector . should_equal [10, x]
            m2.at "Y" . to_vector . should_equal [20, x+10]
            w2 = Problems.expect_only_warning Inexact_Type_Coercion m2
            w2.requested_type . should_equal (Value_Type.Decimal precision=Nothing scale=Nothing)
            w2.actual_type . should_equal Value_Type.Float

            # This has more than 1000 digits.
            super_large = 11^2000
            m3 = Table.new [["X", [super_large]]]
            m3.at "X" . value_type . should_be_a (Value_Type.Decimal ...)
            t3 = m3.select_into_database_table data.connection (Name_Generator.random_name "BigInteger2") primary_key=[] temporary=True
            t3 . at "X" . value_type . should_be_a (Value_Type.Decimal ...)
            # If we exceed the 1000 digits precision, we cannot enforce neither scale nor precision anymore.
            t3 . at "X" . value_type . precision . should_equal Nothing
            t3 . at "X" . value_type . scale . should_equal Nothing
            # Works but only relying on imprecise float equality:
            t3 . at "X" . to_vector . should_equal [super_large]
            w3 = Problems.expect_only_warning Inexact_Type_Coercion t3
            w3.requested_type . should_equal (Value_Type.Decimal precision=Nothing scale=0)
            w3.actual_type . should_equal (Value_Type.Decimal precision=Nothing scale=Nothing)

            m4 = t3.remove_warnings.read
            # Because we no longer have a set scale, we cannot get a BigInteger column back - we'd need BigDecimal, but that is not fully supported yet in Enso - so we get the closest approximation - the imprecise Float.
            m4 . at "X" . value_type . should_equal Value_Type.Float
            m4 . at "X" . to_vector . should_equal [super_large]
            w4 = Problems.expect_only_warning Inexact_Type_Coercion m4
            w4.requested_type . should_equal (Value_Type.Decimal precision=Nothing scale=Nothing)
            w4.actual_type . should_equal Value_Type.Float

        group_builder.specify "should round-trip timestamptz column, preserving instant but converting to UTC" pending="TODO" <|
            table_name = Name_Generator.random_name "TimestampTZ"
            table = data.connection.create_table table_name [Column_Description.Value "A" (Value_Type.Date_Time with_timezone=True)] primary_key=[]

            dt1 = Date_Time.new 2022 05 04 15 30
            dt2 = Date_Time.new 2022 05 04 15 30 zone=(Time_Zone.utc)
            dt3 = Date_Time.new 2022 05 04 15 30 zone=(Time_Zone.parse "US/Hawaii")
            dt4 = Date_Time.new 2022 05 04 15 30 zone=(Time_Zone.parse "Europe/Warsaw")
            v = [dt1, dt2, dt3, dt4]

            Problems.assume_no_problems <|
                table.update_rows (Table.new [["A", v]]) update_action=Update_Action.Insert

            tz_zero = Time_Zone.parse "Z"
            v_at_z = v.map dt-> dt.at_zone tz_zero
            table.at "A" . to_vector . should_equal v_at_z
            table.at "A" . to_vector . should_equal_tz_agnostic v

            ## We also check how the timestamp column behaves with interpolations:
               (See analogous test showing a less nice behaviour without timezones below.)
            v.each my_dt-> Test.with_clue my_dt.to_text+" " <|
                # Checks if the two date-times represent the same instant in time.
                is_same_time_instant dt1 dt2 =
                    dt1.at_zone tz_zero == dt2.at_zone tz_zero
                local_equals = v.filter (is_same_time_instant my_dt)
                # Depending on test runner's timezone, the `my_dt` may be equal to 1 or 2 entries in `v`.
                [1, 2].should_contain local_equals.length

                Test.with_clue " == "+local_equals.to_text+": " <|
                    t2 = table.filter "A" (Filter_Condition.Equal to=my_dt)
                    t2.row_count . should_equal local_equals.length
                    t2.at "A" . to_vector . should_equal_tz_agnostic local_equals

        group_builder.specify "will round-trip timestamp column without timezone by converting it to UTC" pending="TODO" <|
            table_name = Name_Generator.random_name "Timestamp"
            table = data.connection.create_table table_name [Column_Description.Value "A" (Value_Type.Date_Time with_timezone=False)] primary_key=[]
            Problems.assume_no_problems table

            dt1 = Date_Time.new 2022 05 04 15 30
            dt2 = Date_Time.new 2022 05 04 15 30 zone=(Time_Zone.utc)
            dt3 = Date_Time.new 2022 05 04 15 30 zone=(Time_Zone.parse "US/Hawaii")
            dt4 = Date_Time.new 2022 05 04 15 30 zone=(Time_Zone.parse "Europe/Warsaw")
            v = [dt1, dt2, dt3, dt4]

            source_table = Table.new [["A", v]]
            source_table.at "A" . value_type . should_equal (Value_Type.Date_Time with_timezone=True)
            w = Problems.expect_only_warning Inexact_Type_Coercion <|
                table.update_rows source_table update_action=Update_Action.Insert
            w.requested_type . should_equal (source_table.at "A" . value_type)
            w.actual_type . should_equal (table.at "A" . value_type)
            w.to_display_text . should_equal "The type Date_Time (with timezone) has been coerced to Date_Time (without timezone). Some information may be lost."

            # When uploading we want to just strip the timezone information and treat every timestamp as LocalDateTime.
            # This is verified by checking the text representation in the DB: it should show the same local time in all 4 cases, regardless of original timezone.
            local_dt = "2022-05-04 15:30:00"
            table.at "A" . cast Value_Type.Char . to_vector . should_equal [local_dt, local_dt, local_dt, local_dt]

            # Then when downloaded, it should be interpreted at the 'system default' timezone.
            materialized_table = table.read
            materialized_table.at "A" . to_vector . should_equal [dt1, dt1, dt1, dt1]

            # The Inexact_Type_Coercion warning is silenced for this case:
            Problems.assume_no_problems materialized_table

            # We also check how the timestamp column behaves with interpolations:
            v.each my_dt-> Test.with_clue my_dt.to_text+": " <|
                t2 = table.filter "A" (Filter_Condition.Equal to=my_dt)
                is_in_system_tz = my_dt.at_zone Time_Zone.system . time_of_day == my_dt.time_of_day
                ## Unfortunately, this will work in the following way:
                   - if the date-time represented as local time in the current system default timezone is 15:30,
                     then it will match _all_ entries (as they do not have a timezone).
                   - otherwise, the date-time is converted into UTC before being passed to the Database,
                     and only then the timezone is stripped - so the local time is actually shifted.

                   This is not ideal - ideally we'd want the local date time to be extracted from the timestamp directly,
                   before any date conversions happen - this way in _all_ 4 cases we would get 15:30 local time
                   and all rows would always be matching.

                   That logic is applied when uploading a table. However, in custom queries, we do not currently have
                   enough metadata to infer that the Date_Time that is being passed to a given `?` hole in the query
                   should be treated as a local date-time or a zoned date-time.
                   Thus, we pass it as zoned by default to avoid losing information - and that triggers the conversion
                   on the Database side. If we want to change that, we would need to add metadata within our operations,
                   so that an operation like `==` will infer the expected type of the `?` hole based on the type of the
                   second operand.
                case is_in_system_tz of
                    True ->
                        my_dt.at_zone Time_Zone.system . time_of_day . to_display_text . should_equal "15:30:00"
                        t2.row_count . should_equal 4
                        t2.at "A" . to_vector . should_equal [dt1, dt1, dt1, dt1]
                    False ->
                        my_dt.at_zone Time_Zone.system . time_of_day . to_display_text . should_not_equal "15:30:00"
                        t2.row_count . should_equal 0
                        t2.at "A" . to_vector . should_equal []

    suite_builder.group "[Snowflake] math functions" group_builder->
        data = Basic_Test_Data.setup create_connection_fn

        group_builder.teardown <|
            data.teardown

        group_builder.specify "round, trunc, ceil, floor" <|
            col = (table_builder [["x", [0.1, 0.9, 3.1, 3.9, -0.1, -0.9, -3.1, -3.9]]] connection=data.connection) . at "x"
            col . cast Value_Type.Integer . ceil . value_type . should_equal Value_Type.Float

            col . cast Value_Type.Float . round . value_type . should_equal Value_Type.Float
            col . cast Value_Type.Integer . round . value_type . should_equal Value_Type.Float
            col . cast Value_Type.Decimal . round . value_type . should_equal Value_Type.Decimal

            col . cast Value_Type.Float . round 1 . value_type . should_equal Value_Type.Float
            col . cast Value_Type.Integer . round 1 . value_type . should_equal Value_Type.Decimal
            col . cast Value_Type.Decimal . round 1 . value_type . should_equal Value_Type.Decimal

            col . cast Value_Type.Float . round use_bankers=True . value_type . should_equal Value_Type.Float
            col . cast Value_Type.Integer . round use_bankers=True . value_type . should_equal Value_Type.Float
            col . cast Value_Type.Decimal . round use_bankers=True . value_type . should_equal Value_Type.Decimal

            col . cast Value_Type.Float . ceil . value_type . should_equal Value_Type.Float
            col . cast Value_Type.Integer . ceil . value_type . should_equal Value_Type.Float
            col . cast Value_Type.Decimal . ceil . value_type . should_equal Value_Type.Decimal

            col . cast Value_Type.Float . floor . value_type . should_equal Value_Type.Float
            col . cast Value_Type.Integer . floor . value_type . should_equal Value_Type.Float
            col . cast Value_Type.Decimal . floor . value_type . should_equal Value_Type.Decimal

            col . cast Value_Type.Float . truncate . value_type . should_equal Value_Type.Float
            col . cast Value_Type.Integer . truncate . value_type . should_equal Value_Type.Float
            col . cast Value_Type.Decimal . truncate . value_type . should_equal Value_Type.Decimal

        do_op data n op =
            table = table_builder [["x", [n]]] connection=data.connection
            result = table.at "x" |> op
            result.to_vector.at 0

        do_round data n dp=0 use_bankers=False = do_op data n (_.round dp use_bankers)

        group_builder.specify "Can round correctly near the precision limit" <|
            do_round data 1.2222222222222225 15 . should_equal 1.222222222222223
            do_round data -1.2222222222222225 15 . should_equal -1.222222222222223
            do_round data 1.2222222222222235 15 . should_equal 1.222222222222224
            do_round data -1.2222222222222235 15 . should_equal -1.222222222222224

        group_builder.specify "Can round correctly near the precision limit, using banker's rounding" <|
            do_round data 1.2222222222222225 15 use_bankers=True . should_equal 1.222222222222222
            do_round data -1.2222222222222225 15 use_bankers=True . should_equal -1.222222222222222
            do_round data 1.2222222222222235 15 use_bankers=True . should_equal 1.222222222222224
            do_round data -1.2222222222222235 15 use_bankers=True . should_equal -1.222222222222224

        group_builder.specify "Can handle NaN/Infinity" <|
            nan_result = if setup.test_selection.is_nan_and_nothing_distinct then Number.nan else Nothing
            ops = [.round, .truncate, .ceil, .floor]
            ops.each op->
                do_op data Number.nan op . should_equal nan_result
                do_op data Number.positive_infinity op . should_equal Number.positive_infinity
                do_op data Number.negative_infinity op . should_equal Number.negative_infinity

        group_builder.specify "round returns the correct type" <|
            do_round data 231.2 1 . should_be_a Float
            do_round data 231.2 0 . should_be_a Float
            do_round data 231.2 . should_be_a Float
            do_round data 231.2 -1 . should_be_a Float

        group_builder.specify "round returns the correct type" <|
            do_round data 231 1 . should_be_a Float
            do_round data 231 0 . should_be_a Float
            do_round data 231 . should_be_a Float
            do_round data 231 -1 . should_be_a Float

add_snowflake_specs suite_builder create_connection_fn db_name =
    prefix = "[Snowflake] "
    name_counter = Ref.new 0

    table_builder columns connection=(create_connection_fn Nothing) =
        ix = name_counter.get
        name_counter . put ix+1
        name = Name_Generator.random_name "table_"+ix.to_text

        in_mem_table = Table.new columns
        in_mem_table.select_into_database_table connection name primary_key=Nothing temporary=True

    materialize = .read

    Common_Spec.add_specs suite_builder prefix create_connection_fn

    common_selection = Common_Table_Operations.Main.Test_Selection.Config supports_case_sensitive_columns=True order_by_unicode_normalization_by_default=True allows_mixed_type_comparisons=False fixed_length_text_columns=True removes_trailing_whitespace_casting_from_char_to_varchar=True supports_decimal_type=True supported_replace_params=supported_replace_params
    aggregate_selection = Common_Table_Operations.Aggregate_Spec.Test_Selection.Config first_last_row_order=False aggregation_problems=False
    agg_in_memory_table = ((Project_Description.new enso_dev.Table_Tests).data / "data.csv") . read

    agg_table_fn = _->
        connection = create_connection_fn Nothing
        agg_in_memory_table.select_into_database_table connection (Name_Generator.random_name "Agg1") primary_key=Nothing temporary=True

    empty_agg_table_fn = _->
        connection = create_connection_fn Nothing
        (agg_in_memory_table.take (First 0)).select_into_database_table connection (Name_Generator.random_name "Agg_Empty") primary_key=Nothing temporary=True

    setup = Common_Table_Operations.Main.Test_Setup.Config prefix agg_table_fn empty_agg_table_fn table_builder materialize is_database=True test_selection=common_selection aggregate_test_selection=aggregate_selection create_connection_func=create_connection_fn

    snowflake_specific_spec suite_builder create_connection_fn db_name setup
    Common_Table_Operations.Main.add_specs suite_builder setup

## PRIVATE
supported_replace_params : Set Replace_Params
supported_replace_params =
    e0 = [Replace_Params.Value Text Case_Sensitivity.Default False, Replace_Params.Value Text Case_Sensitivity.Default True, Replace_Params.Value Text Case_Sensitivity.Sensitive False]
    e1 = [Replace_Params.Value Text Case_Sensitivity.Sensitive True, Replace_Params.Value Text Case_Sensitivity.Insensitive False, Replace_Params.Value Text Case_Sensitivity.Insensitive True]
    e2 = [Replace_Params.Value Regex Case_Sensitivity.Default False, Replace_Params.Value Regex Case_Sensitivity.Default True, Replace_Params.Value Regex Case_Sensitivity.Sensitive False]
    e3 = [Replace_Params.Value Regex Case_Sensitivity.Sensitive True, Replace_Params.Value Regex Case_Sensitivity.Insensitive False, Replace_Params.Value Regex Case_Sensitivity.Insensitive True]
    e4 = [Replace_Params.Value DB_Column Case_Sensitivity.Default False, Replace_Params.Value DB_Column Case_Sensitivity.Sensitive False]
    Set.from_vector <| e0 + e1 + e2 + e3 + e4

add_table_specs suite_builder =
    case create_connection_builder of
        Nothing ->
            message = "Snowflake test connection is not configured. See README.md for instructions."
            suite_builder.group "[Snowflake] Database tests" pending=message (_-> Nothing)
        connection_builder ->
            db_name = get_configured_connection_details.database
            add_snowflake_specs suite_builder connection_builder db_name
            Transaction_Spec.add_specs suite_builder connection_builder "[Snowflake] "
            Upload_Spec.add_specs suite_builder connection_builder "[Snowflake] "

            suite_builder.group "[Snowflake] Secrets in connection settings" group_builder->
                cloud_setup = Cloud_Tests_Setup.prepare
                base_details = get_configured_connection_details
                group_builder.specify "should allow to set up a connection with the password passed as a secret" pending=cloud_setup.pending <|
                    cloud_setup.with_prepared_environment <|
                        with_secret "my_postgres_username" base_details.credentials.username username_secret-> with_secret "my_postgres_password" base_details.credentials.password password_secret->
                            secret_credentials = Credentials.Username_And_Password username_secret password_secret
                            details = Snowflake_Details.Snowflake base_details.account_name secret_credentials base_details.database base_details.schema base_details.warehouse
                            connection = Database.connect details
                            connection.should_succeed
                            Panic.with_finalizer connection.close <|
                                connection.tables . should_be_a Table

with_secret name value callback =
    secret = Enso_Secret.create name+Random.uuid value
    secret.should_succeed
    Panic.with_finalizer secret.delete (callback secret)

get_configured_connection_details =
    account_name = Environment.get "ENSO_SNOWFLAKE_ACCOUNT"
    if account_name.is_nothing then Nothing else
        get_var name =
            Environment.get name if_missing=(Panic.throw (Illegal_State.Error "ENSO_SNOWFLAKE_ACCOUNT is set, but "+name+" is not. Please set all required environment variables."))
        user = get_var "ENSO_SNOWFLAKE_USER"
        password = get_var "ENSO_SNOWFLAKE_PASSWORD"
        database = get_var "ENSO_SNOWFLAKE_DATABASE"
        schema = Environment.get "ENSO_SNOWFLAKE_SCHEMA" if_missing="PUBLIC"
        warehouse = Environment.get "ENSO_SNOWFLAKE_WAREHOUSE" if_missing=""

        resolved_password = if password.starts_with "enso://" then Enso_Secret.get password else password
        credentials = Credentials.Username_And_Password user resolved_password
        Snowflake_Details.Snowflake account_name credentials database schema warehouse

## Returns a function that takes anything and returns a new connection.
   The function creates a _new_ connection on each invocation
   (this is needed for some tests that need multiple distinct connections).
create_connection_builder =
    connection_details = get_configured_connection_details
    connection_details.if_not_nothing <|
        _ -> Database.connect connection_details

add_specs suite_builder =
    add_table_specs suite_builder

main filter=Nothing =
    suite = Test.build suite_builder->
        add_specs suite_builder
    suite.run_with_filter filter
